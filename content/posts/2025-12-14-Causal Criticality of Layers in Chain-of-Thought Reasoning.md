+++
title = "Causal Criticality of Layers in Chain-of-Thought (CoT) — a Quick Dive"
date = 2025-12-14
summary = "Late layers matter. A probe into Qwen2-1.5B with activation patching, a logit lens, and targeted ablations found that the model's mid-to-late layers — especially around layer 24 — often synthesize intermediate chain-of-thought steps into the final answer."
tags = ["Chain-of-Thought", "CoT", "Interpretability", "LLM", "AI", "Qwen2"]
+++

Short version: late layers matter. I probed Qwen2-1.5B with activation patching, a logit lens, and targeted ablations, and found that the model's mid-to-late layers — especially around layer 24 — often synthesize intermediate chain-of-thought steps into the final answer while earlier layers do much of the foundational computation.

Chain-of-thought (CoT) gives models a way to reason out loud, but that raises a practical question: which parts of the network actually make the final call? I set out to answer that by selecting six GSM8K-style math problems where Qwen2-1.5B sometimes stumbles, then running two baselines and a targeted intervention. In the “noisy clean” baseline I corrupted early activations with Gaussian noise (σ=2.0) to break the chain; in the base run I cached activations at key source layers (16, 20, 24). During the noisy run I restored residuals from the base run into the late positions of layer 24 (last 50 tokens) and measured logit improvements for the correct token. That patching experiment was paired with a layer-wise logit lens that projects residuals to logits across layers and token positions, plus a few ablations (zeroing the layer-24 MLP) and neuron probes (notably neuron 609).

The results were consistent enough to be interesting. Patching into layer 24 produced the largest average logit improvement for the correct token (≈ +1.48), while layer 20 had a moderate positive effect (≈ +1.33). Patching from layer 16 was inconsistent and often harmful (avg ≈ -2.23), suggesting that not all residuals are interchangeable and that early/mid-layer interventions can introduce interference. Zero-ablating the MLP in layer 24 usually made things worse (avg logit change ≈ -0.20), and the logit-lens heatmaps show recurring spikes at positions like 50, 100, 150… where correct-token probability often “crystallizes” in mid-to-late layers.



![Fig.1 — How the model’s belief in the correct answer evolves across layers and token positions.](https://raw.githubusercontent.com/blueee04/blog/refs/heads/main/content/images/2025-12-14-Causal%20Criticality%20of%20Layers%20in%20Chain-of-Thought%20Reasoning/Logit%20Lens%20Probability%20Heatmap_wrong%20and%20right%20answer.png)

![Fig.2 — Patch improvements (Δlogit) by source layer showing layer-24 rescue effect.](https://raw.githubusercontent.com/blueee04/blog/refs/heads/main/content/images/2025-12-14-Causal%20Criticality%20of%20Layers%20in%20Chain-of-Thought%20Reasoning/avg_improvement_in_correct_token_logit.png)

![Fig.3 — Activation trace for neuron 609, which spikes on math tokens in late positions.](https://raw.githubusercontent.com/blueee04/blog/refs/heads/main/content/images/2025-12-14-Causal%20Criticality%20of%20Layers%20in%20Chain-of-Thought%20Reasoning/Neuron%20609%20activation%20pattern.png)

A brief methods note: the experiments used Qwen2-1.5B on CUDA with torch (bfloat16). Prompts followed the simple template `Q: {question}\nA:` to elicit natural CoT. The logit-lens data and per-example arrays were saved to a `.pkl` (structure: per-example dicts with `layer_probs_correct`, `layer_probs_wrong`, `cot_length`, and metadata), and visualizations were generated by truncating sequences to a common length for averaging and plotting heatmaps and bar charts.

Limitations: this was a quick exploratory sweep over a small set of problems (six core patching cases, ~100 aggregated CoTs for the logit lens). Noise-level choices (σ=2.0) were tuned empirically and may need calibration. The conclusions point to layered specialization in CoT, but they are preliminary and task-specific.

