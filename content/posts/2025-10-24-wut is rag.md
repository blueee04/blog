+++
title = "How does ChatGPT answer everything we ever ask "
date = 2025-10-24
summary = "Ever wondered how does ChatGpt have an answer to everything???"
tags = ["n8n", "automation", "job-search", "AI", "productivity"]
+++

In the models nowadays, reponses are generated mostly based on the pre-learned data and paterns during the training phase. However these models are limited to the data they were trained on, often leading to responses that don't make any sense and is rubbish.

Here comes in a Super Power called RAG(Retrieval Augmented Generation) which addresses this limitation by fetching external data from the model whenever needed.

When a model doesnot know any context on what it is generating this scenario is called Halluciantions. It creates patterns or objects that are non-existent

RAG ensures that the llms does not solely rely on static training data, It helps retrieve relevant documents at the query time and feeds them into the model as context.
By recieving the new and context-aware data, AI can generate more accurate, current and domain-specific responses


Now how does this RAG work???

When a user submits a query, the system first engages the retieval model, which uses a vector database to identify and "retrieve" semantically similar documents, databases or other sources for relevant information.
Once identified, it then combines those results with the original input prompt and sends it to a generative AI model, which synthesizes the new information into its own model.

This allows the LLM to produce more accurate, context-aware answers grounded in enterprise-specific or up-to-date data, rather than simply relying upon the model it was trained on.

Here's a good comparision chart i found between prompt engineering vs. RAG vs. fine-tune vs. pretrain

Now you guys must think what is this vector space and embedding i am talking about

In vector search, data and queries are represented as vectors in a multi-dimensional space called embeddings from a Generative AI model. 
Let’s take a simple example where we want to use vector search to find semantically similar words in a large corpus of words. So, if you query the corpus with the word ‘dog’, you want words like ‘puppy’ to be returned. But, if you search for ‘car’, you want to retrieve words like ‘van’. In traditional search, you will have to maintain a list of synonyms or “similar words” which is hard to generate or scale. In order to use vector search, you can instead use a Generative AI model to convert these words into vectors in a n-dimensional space called embeddings. These vectors will have the property that semantically similar words like ‘dog’ and ‘puppy’ will be closer to each in the n-dimensional space than the words ‘dog’ and ‘car’.

Now how do we calculate how similar the query is to the target document/context???

Cosine Similarity is the most typically used metrics which measures the cosine of the angle between the two vectors.