<!DOCTYPE html>
<html lang="en" dir="auto">

<head><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="index, follow">
<title>Before We Hop Onto TRANSFORMERS  | bluee :)</title>
<meta name="keywords" content="RAG, retrieval-augmented-generation, embeddings, vector-search, AI">
<meta name="description" content="Understanding The Transformer.(You Don&rsquo;t Wanna Skip This)">
<meta name="author" content="Barshan Mondal">
<link rel="canonical" href="https://barshan.is-a.dev/posts/2025-11-19-transfomers/">
<link crossorigin="anonymous" href="/assets/css/stylesheet.b609c58d5c11bb90b1a54e04005d74ad1ddf22165eb79f5533967e57df9c3b50.css" integrity="sha256-tgnFjVwRu5CxpU4EAF10rR3fIhZet59VM5Z&#43;V9&#43;cO1A=" rel="preload stylesheet" as="style">
<link rel="icon" href="https://barshan.is-a.dev/favicon.ico">
<link rel="icon" type="image/png" sizes="16x16" href="https://barshan.is-a.dev/favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="https://barshan.is-a.dev/favicon-32x32.png">
<link rel="apple-touch-icon" href="https://barshan.is-a.dev/apple-touch-icon.png">
<link rel="mask-icon" href="https://barshan.is-a.dev/safari-pinned-tab.svg">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
    <style>
        @media (prefers-color-scheme: dark) {
            :root {
                --theme: rgb(29, 30, 32);
                --entry: rgb(46, 46, 51);
                --primary: rgb(218, 218, 219);
                --secondary: rgb(155, 156, 157);
                --tertiary: rgb(65, 66, 68);
                --content: rgb(196, 196, 197);
                --code-block-bg: rgb(46, 46, 51);
                --code-bg: rgb(55, 56, 62);
                --border: rgb(51, 51, 51);
            }

            .list {
                background: var(--theme);
            }

            .list:not(.dark)::-webkit-scrollbar-track {
                background: 0 0;
            }

            .list:not(.dark)::-webkit-scrollbar-thumb {
                border-color: var(--theme);
            }
        }

    </style>
</noscript><meta property="og:title" content="Before We Hop Onto TRANSFORMERS " />
<meta property="og:description" content="Understanding The Transformer.(You Don&rsquo;t Wanna Skip This)" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://barshan.is-a.dev/posts/2025-11-19-transfomers/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2025-11-19T00:00:00+00:00" />
<meta property="article:modified_time" content="2025-11-19T00:00:00+00:00" />

<meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="Before We Hop Onto TRANSFORMERS "/>
<meta name="twitter:description" content="Understanding The Transformer.(You Don&rsquo;t Wanna Skip This)"/>


<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [
    {
      "@type": "ListItem",
      "position":  1 ,
      "name": "Posts",
      "item": "https://barshan.is-a.dev/posts/"
    }, 
    {
      "@type": "ListItem",
      "position":  2 ,
      "name": "Before We Hop Onto TRANSFORMERS ",
      "item": "https://barshan.is-a.dev/posts/2025-11-19-transfomers/"
    }
  ]
}
</script>
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "Before We Hop Onto TRANSFORMERS ",
  "name": "Before We Hop Onto TRANSFORMERS ",
  "description": "Understanding The Transformer.(You Don\u0026rsquo;t Wanna Skip This)",
  "keywords": [
    "RAG", "retrieval-augmented-generation", "embeddings", "vector-search", "AI"
  ],
  "articleBody": "Okay so as i baseline i’ll be taking the GPT-2 style transformers. Now what do they do….They generate text,the user feeds in language and the model generates a proability distribution over the tokens and this is repeatedly sampled to generate more texts ahead\nWe can call this type as “Auto-Regressive” since it predicts the future based only on past data.\nHere’s a cool analogy i found where we have a series of people standing in a line, each with one word or chunk of the sentence. Each person has the ability to look up information from the people behind them (we’ll explore how this works in later sections) but they can’t look at any information in front of them. Their goal is to guess what word the person in front of them is holding.\nInputs Now the model cannot take language as input they take vectors so how do we do that?\nWe can factor this into 2 questions:\nHow do we split up language into small sub-units? How do we convert these sub-units into vectors? Let’s start with the second of these questions :\nWe basically make a massive lookup table, which is called an embedding. It has one vector for each possible sub-unit of language we might get (we call this set of all sub-units our vocabulary). We label every element in our vocabulary with an integer (this labelling never changes), and we use this integer to index into the embedding.\nNow there are a lot of ways to do this encoding some of them are:\nOne-Hot-Encoding (To Vectors) Byte Pair Encoding (To Sub-Units) I won’t be explaining these one by one but you can look them up through the hyper link above\nText Generation Now that we understand the basic idea we can now go through the entire process of text generation from our original string to a new token which we can append to our string and plug back into the model\nStep 1 : Converting The Text To Tokens reference_text = \"I am an amazing autoregressive, decoder-only, GPT-2 style transformer. One day I will exceed human level intelligence and take over the world!\" tokens = reference_gpt2.to_tokens(reference_text).to(device) print(tokens) print(tokens.shape) print(reference_gpt2.to_str_tokens(tokens)) tensor([[50256, 40, 716, 281, 4998, 1960, 382, 19741, 11, 875, 12342, 12, 8807, 11, 402, 11571, 12, 17, 3918, 47385, 13, 1881, 1110, 314, 481, 7074, 1692, 1241, 4430, 290, 1011, 625, 262, 995, 0]], device=‘cuda:0’) torch.Size([1, 35]) [’\u003c|endoftext|\u003e’, ‘I’, ’ am’, ’ an’, ’ amazing’, ’ aut’, ‘ore’, ‘gressive’, ‘,’, ’ dec’, ‘oder’, ‘-’, ‘only’, ‘,’, ’ G’, ‘PT’, ‘-’, ‘2’, ’ style’, ’ transformer’, ‘.’, ’ One’, ’ day’, ’ I’, ’ will’, ’ exceed’, ’ human’, ’ level’, ’ intelligence’, ’ and’, ’ take’, ’ over’, ’ the’, ’ world’, ‘!’]\nMapping The Tokens To Logits logits, cache = reference_gpt2.run_with_cache(tokens) print(logits.shape) torch.Size([1, 35, 50257])\nConvert The Logits to a distribution with Softmax probs = logits.softmax(dim=-1) print(probs.shape) torch.Size([1, 35, 50257])\nMap Distribution To A Token next_token = logits[0, -1].argmax(dim=-1) next_char = reference_gpt2.to_string(next_token) print(repr(next_char)) ’ I’\nNote that we’re indexing logits[0, -1]. This is because logits have shape [1, sequence_length, vocab_size], so this indexing returns the vector of length vocab_size representing the model’s prediction for what token follows the last token in the input sequence.\nIn this case, we can see that the model predicts the token ’ I’.\nAdd this to the end of the input, re-run There are more efficient ways to do this (e.g. where we cache some of the values each time we run our input, so we don’t have to do as much calculation each time we generate a new value), but this doesn’t matter conceptually right now.\nprint(f\"Sequence so far: {reference_gpt2.to_string(tokens)[0]!r}\") for i in range(10): print(f\"{tokens.shape[-1] + 1}th char = {next_char!r}\") # Define new input sequence, by appending the previously generated token tokens = t.cat([tokens, next_token[None, None]], dim=-1) # Pass our new sequence through the model, to get new output logits = reference_gpt2(tokens) # Get the predicted token at the end of our sequence next_token = logits[0, -1].argmax(dim=-1) # Decode and print the result next_char = reference_gpt2.to_string(next_token) Sequence so far: ‘\u003c|endoftext|\u003eI am an amazing autoregressive, decoder-only, GPT-2 style transformer. One day I will exceed human level intelligence and take over the world!’ 36th char = ’ I’ 37th char = ’ am’ 38th char = ’ a’ 39th char = ’ very’ 40th char = ’ talented’ 41th char = ’ and’ 42th char = ’ talented’ 43th char = ’ person’ 44th char = ‘,’ 45th char = ’ and’\nSo here are a few takeaways that you should keep in your mind:\nTransformer takes in language, predicts next token (for each token in a causal way) We convert language to a sequence of integers with a tokenizer. We convert integers to vectors with a lookup table. Output is a vector of logits (one for each input token), we convert to a probability distn with a softmax, and can then convert this to a token (eg taking the largest logit, or sampling). We append this to the input + run again to generate more text (Jargon: autoregressive) Meta level point: Transformers are sequence operation models, they take in a sequence, do processing in parallel at each position, and use attention to move information between positions! In my next writeup i’ll be cooking a transformer from scratch so make sure to stick around :).\n",
  "wordCount" : "895",
  "inLanguage": "en",
  "datePublished": "2025-11-19T00:00:00Z",
  "dateModified": "2025-11-19T00:00:00Z",
  "author":{
    "@type": "Person",
    "name": "Barshan Mondal"
  },
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "https://barshan.is-a.dev/posts/2025-11-19-transfomers/"
  },
  "publisher": {
    "@type": "Organization",
    "name": "bluee :)",
    "logo": {
      "@type": "ImageObject",
      "url": "https://barshan.is-a.dev/favicon.ico"
    }
  }
}
</script>
</head>

<body class="" id="top">
<script>
    if (localStorage.getItem("pref-theme") === "dark") {
        document.body.classList.add('dark');
    } else if (localStorage.getItem("pref-theme") === "light") {
        document.body.classList.remove('dark')
    } else if (window.matchMedia('(prefers-color-scheme: dark)').matches) {
        document.body.classList.add('dark');
    }

</script>

<header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="https://barshan.is-a.dev/" accesskey="h" title="bluee :) (Alt + H)">bluee :)</a>
            <div class="logo-switches">
                <button id="theme-toggle" accesskey="t" title="(Alt + T)">
                    <svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
                    </svg>
                    <svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <circle cx="12" cy="12" r="5"></circle>
                        <line x1="12" y1="1" x2="12" y2="3"></line>
                        <line x1="12" y1="21" x2="12" y2="23"></line>
                        <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
                        <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
                        <line x1="1" y1="12" x2="3" y2="12"></line>
                        <line x1="21" y1="12" x2="23" y2="12"></line>
                        <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
                        <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
                    </svg>
                </button>
            </div>
        </div>
        <ul id="menu">
            <li>
                <a href="https://barshan.is-a.dev/tags/" title="Tags">
                    <span>Tags</span>
                </a>
            </li>
        </ul>
    </nav>
</header>
<main class="main">

<article class="post-single">
  <header class="post-header">
    <div class="breadcrumbs"><a href="https://barshan.is-a.dev/">Home</a>&nbsp;»&nbsp;<a href="https://barshan.is-a.dev/posts/">Posts</a></div>
    <h1 class="post-title entry-hint-parent">
      Before We Hop Onto TRANSFORMERS 
    </h1>
    <div class="post-meta"><span title='2025-11-19 00:00:00 +0000 UTC'>November 19, 2025</span>&nbsp;·&nbsp;5 min&nbsp;·&nbsp;Barshan Mondal

</div>
  </header> <div class="toc">
    <details >
        <summary accesskey="c" title="(Alt + C)">
            <span class="details">Table of Contents</span>
        </summary>

        <div class="inner"><ul>
                <li>
                    <a href="#inputs" aria-label="Inputs">Inputs</a></li>
                <li>
                    <a href="#text-generation" aria-label="Text Generation">Text Generation</a><ul>
                        
                <li>
                    <a href="#step-1--converting-the-text-to-tokens" aria-label="Step 1 : Converting The Text To Tokens">Step 1 : Converting The Text To Tokens</a></li>
                <li>
                    <a href="#mapping-the-tokens-to-logits" aria-label="Mapping The Tokens To Logits">Mapping The Tokens To Logits</a></li>
                <li>
                    <a href="#convert-the-logits-to-a-distribution-with-softmax" aria-label="Convert The Logits to a distribution with Softmax">Convert The Logits to a distribution with Softmax</a></li>
                <li>
                    <a href="#map-distribution-to-a-token" aria-label="Map Distribution To A Token">Map Distribution To A Token</a></li>
                <li>
                    <a href="#add-this-to-the-end-of-the-input-re-run" aria-label="Add this to the end of the input, re-run">Add this to the end of the input, re-run</a>
                </li>
            </ul>
            </li>
            </ul>
        </div>
    </details>
</div>

  <div class="post-content"><p>Okay so as i baseline i&rsquo;ll be taking the GPT-2 style transformers. Now what do they do&hellip;.They generate text,the user feeds in language and the model generates a proability distribution over the tokens and this is repeatedly sampled to generate more texts ahead</p>
<p>We can call this type as &ldquo;Auto-Regressive&rdquo; since it predicts the future based only on past data.</p>
<p><img loading="lazy" src="https://raw.githubusercontent.com/blueee04/blog/refs/heads/main/content/images/2025-09-19-Transformers/transformer-overview-new.png" alt="Transformer Overview"  />
</p>
<p>Here&rsquo;s a cool analogy i found where we have a series of people standing in a line, each with one word or chunk of the sentence. Each person has the ability to look up information from the people behind them (we&rsquo;ll explore how this works in later sections) but they can&rsquo;t look at any information in front of them. Their goal is to guess what word the person in front of them is holding.</p>
<p><img loading="lazy" src="https://raw.githubusercontent.com/blueee04/blog/refs/heads/main/content/images/2025-09-19-Transformers/intro-image-v2.png" alt="Cool Analogy"  />
</p>
<h2 id="inputs">Inputs<a hidden class="anchor" aria-hidden="true" href="#inputs">#</a></h2>
<p>Now the model cannot take language as input they take vectors so how do we do that?</p>
<p>We can factor this into 2 questions:</p>
<ul>
<li>How do we split up language into small sub-units?</li>
<li>How do we convert these sub-units into vectors?</li>
</ul>
<p>Let&rsquo;s start with the second of these questions :</p>
<p>We basically make a massive lookup table, which is called an embedding. It has one vector for each possible sub-unit of language we might get (we call this set of all sub-units our vocabulary). We label every element in our vocabulary with an integer (this labelling never changes), and we use this integer to index into the embedding.</p>
<p>Now there are a lot of ways to do this encoding some of them are:</p>
<ul>
<li>One-Hot-Encoding (To Vectors)</li>
<li>Byte Pair Encoding (To Sub-Units)</li>
</ul>
<p>I won&rsquo;t be explaining these one by one but you can look them up through the hyper link above</p>
<h2 id="text-generation">Text Generation<a hidden class="anchor" aria-hidden="true" href="#text-generation">#</a></h2>
<p>Now that we understand the basic idea we can now go through the entire process of text generation from our original string to a new token which we can append to our string and plug back into the model</p>
<h3 id="step-1--converting-the-text-to-tokens">Step 1 : Converting The Text To Tokens<a hidden class="anchor" aria-hidden="true" href="#step-1--converting-the-text-to-tokens">#</a></h3>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="n">reference_text</span> <span class="o">=</span> <span class="s2">&#34;I am an amazing autoregressive, decoder-only, GPT-2 style transformer. One day I will exceed human level intelligence and take over the world!&#34;</span>
</span></span><span class="line"><span class="cl"><span class="n">tokens</span> <span class="o">=</span> <span class="n">reference_gpt2</span><span class="o">.</span><span class="n">to_tokens</span><span class="p">(</span><span class="n">reference_text</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="nb">print</span><span class="p">(</span><span class="n">tokens</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="nb">print</span><span class="p">(</span><span class="n">tokens</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="nb">print</span><span class="p">(</span><span class="n">reference_gpt2</span><span class="o">.</span><span class="n">to_str_tokens</span><span class="p">(</span><span class="n">tokens</span><span class="p">))</span>
</span></span></code></pre></div><blockquote>
<p>tensor([[50256,    40,   716,   281,  4998,  1960,   382, 19741,    11,   875,
12342,    12,  8807,    11,   402, 11571,    12,    17,  3918, 47385,
13,  1881,  1110,   314,   481,  7074,  1692,  1241,  4430,   290,
1011,   625,   262,   995,     0]], device=&lsquo;cuda:0&rsquo;)
torch.Size([1, 35])
[&rsquo;&lt;|endoftext|&gt;&rsquo;, &lsquo;I&rsquo;, &rsquo; am&rsquo;, &rsquo; an&rsquo;, &rsquo; amazing&rsquo;, &rsquo; aut&rsquo;, &lsquo;ore&rsquo;, &lsquo;gressive&rsquo;, &lsquo;,&rsquo;, &rsquo; dec&rsquo;, &lsquo;oder&rsquo;, &lsquo;-&rsquo;, &lsquo;only&rsquo;, &lsquo;,&rsquo;, &rsquo; G&rsquo;, &lsquo;PT&rsquo;, &lsquo;-&rsquo;, &lsquo;2&rsquo;, &rsquo; style&rsquo;, &rsquo; transformer&rsquo;, &lsquo;.&rsquo;, &rsquo; One&rsquo;, &rsquo; day&rsquo;, &rsquo; I&rsquo;, &rsquo; will&rsquo;, &rsquo; exceed&rsquo;, &rsquo; human&rsquo;, &rsquo; level&rsquo;, &rsquo; intelligence&rsquo;, &rsquo; and&rsquo;, &rsquo; take&rsquo;, &rsquo; over&rsquo;, &rsquo; the&rsquo;, &rsquo; world&rsquo;, &lsquo;!&rsquo;]</p>
</blockquote>
<h3 id="mapping-the-tokens-to-logits">Mapping The Tokens To Logits<a hidden class="anchor" aria-hidden="true" href="#mapping-the-tokens-to-logits">#</a></h3>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="n">logits</span><span class="p">,</span> <span class="n">cache</span> <span class="o">=</span> <span class="n">reference_gpt2</span><span class="o">.</span><span class="n">run_with_cache</span><span class="p">(</span><span class="n">tokens</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="nb">print</span><span class="p">(</span><span class="n">logits</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
</span></span></code></pre></div><blockquote>
<p>torch.Size([1, 35, 50257])</p>
</blockquote>
<h3 id="convert-the-logits-to-a-distribution-with-softmax">Convert The Logits to a distribution with Softmax<a hidden class="anchor" aria-hidden="true" href="#convert-the-logits-to-a-distribution-with-softmax">#</a></h3>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="n">probs</span> <span class="o">=</span> <span class="n">logits</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="nb">print</span><span class="p">(</span><span class="n">probs</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
</span></span></code></pre></div><blockquote>
<p>torch.Size([1, 35, 50257])</p>
</blockquote>
<h3 id="map-distribution-to-a-token">Map Distribution To A Token<a hidden class="anchor" aria-hidden="true" href="#map-distribution-to-a-token">#</a></h3>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="n">next_token</span> <span class="o">=</span> <span class="n">logits</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">next_char</span> <span class="o">=</span> <span class="n">reference_gpt2</span><span class="o">.</span><span class="n">to_string</span><span class="p">(</span><span class="n">next_token</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="nb">print</span><span class="p">(</span><span class="nb">repr</span><span class="p">(</span><span class="n">next_char</span><span class="p">))</span>
</span></span></code></pre></div><blockquote>
<p>&rsquo; I&rsquo;</p>
</blockquote>
<p>Note that we&rsquo;re indexing logits[0, -1]. This is because logits have shape [1, sequence_length, vocab_size], so this indexing returns the vector of length vocab_size representing the model&rsquo;s prediction for what token follows the last token in the input sequence.</p>
<p>In this case, we can see that the model predicts the token &rsquo; I&rsquo;.</p>
<h3 id="add-this-to-the-end-of-the-input-re-run">Add this to the end of the input, re-run<a hidden class="anchor" aria-hidden="true" href="#add-this-to-the-end-of-the-input-re-run">#</a></h3>
<p>There are more efficient ways to do this (e.g. where we cache some of the values each time we run our input, so we don&rsquo;t have to do as much calculation each time we generate a new value), but this doesn&rsquo;t matter conceptually right now.</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&#34;Sequence so far: </span><span class="si">{</span><span class="n">reference_gpt2</span><span class="o">.</span><span class="n">to_string</span><span class="p">(</span><span class="n">tokens</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span><span class="si">!r}</span><span class="s2">&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">10</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&#34;</span><span class="si">{</span><span class="n">tokens</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">+</span> <span class="mi">1</span><span class="si">}</span><span class="s2">th char = </span><span class="si">{</span><span class="n">next_char</span><span class="si">!r}</span><span class="s2">&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># Define new input sequence, by appending the previously generated token</span>
</span></span><span class="line"><span class="cl">    <span class="n">tokens</span> <span class="o">=</span> <span class="n">t</span><span class="o">.</span><span class="n">cat</span><span class="p">([</span><span class="n">tokens</span><span class="p">,</span> <span class="n">next_token</span><span class="p">[</span><span class="kc">None</span><span class="p">,</span> <span class="kc">None</span><span class="p">]],</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># Pass our new sequence through the model, to get new output</span>
</span></span><span class="line"><span class="cl">    <span class="n">logits</span> <span class="o">=</span> <span class="n">reference_gpt2</span><span class="p">(</span><span class="n">tokens</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># Get the predicted token at the end of our sequence</span>
</span></span><span class="line"><span class="cl">    <span class="n">next_token</span> <span class="o">=</span> <span class="n">logits</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># Decode and print the result</span>
</span></span><span class="line"><span class="cl">    <span class="n">next_char</span> <span class="o">=</span> <span class="n">reference_gpt2</span><span class="o">.</span><span class="n">to_string</span><span class="p">(</span><span class="n">next_token</span><span class="p">)</span>
</span></span></code></pre></div><blockquote>
<p>Sequence so far: &lsquo;&lt;|endoftext|&gt;I am an amazing autoregressive, decoder-only, GPT-2 style transformer. One day I will exceed human level intelligence and take over the world!&rsquo;
36th char = &rsquo; I&rsquo;
37th char = &rsquo; am&rsquo;
38th char = &rsquo; a&rsquo;
39th char = &rsquo; very&rsquo;
40th char = &rsquo; talented&rsquo;
41th char = &rsquo; and&rsquo;
42th char = &rsquo; talented&rsquo;
43th char = &rsquo; person&rsquo;
44th char = &lsquo;,&rsquo;
45th char = &rsquo; and&rsquo;</p>
</blockquote>
<p>So here are a few takeaways that you should keep in your mind:</p>
<ul>
<li>Transformer takes in language, predicts next token (for each token in a causal way)</li>
<li>We convert language to a sequence of integers with a tokenizer.</li>
<li>We convert integers to vectors with a lookup table.</li>
<li>Output is a vector of logits (one for each input token), we convert to a probability distn with a softmax, and can then convert this to a token (eg taking the largest logit, or sampling).</li>
<li>We append this to the input + run again to generate more text (Jargon: autoregressive)</li>
<li>Meta level point: Transformers are sequence operation models, they take in a sequence, do processing in parallel at each position, and use attention to move information between positions!</li>
</ul>
<p>In my next writeup i&rsquo;ll be cooking a transformer from scratch so make sure to stick around :).</p>


  </div>

  <footer class="post-footer">
    <ul class="post-tags">
      <li><a href="https://barshan.is-a.dev/tags/rag/">RAG</a></li>
      <li><a href="https://barshan.is-a.dev/tags/retrieval-augmented-generation/">retrieval-augmented-generation</a></li>
      <li><a href="https://barshan.is-a.dev/tags/embeddings/">embeddings</a></li>
      <li><a href="https://barshan.is-a.dev/tags/vector-search/">vector-search</a></li>
      <li><a href="https://barshan.is-a.dev/tags/ai/">AI</a></li>
    </ul>
<nav class="paginav">
  <a class="prev" href="https://barshan.is-a.dev/posts/2025-12-14-causal-criticality-of-layers-in-chain-of-thought-reasoning/">
    <span class="title">« Prev</span>
    <br>
    <span>Causal Criticality of Layers in Chain-of-Thought (CoT) — a Quick Dive</span>
  </a>
  <a class="next" href="https://barshan.is-a.dev/posts/2025-09-14-about-me/">
    <span class="title">Next »</span>
    <br>
    <span>Just a little overview on me</span>
  </a>
</nav>


<ul class="share-buttons">
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Before We Hop Onto TRANSFORMERS  on x"
            href="https://x.com/intent/tweet/?text=Before%20We%20Hop%20Onto%20TRANSFORMERS%20&amp;url=https%3a%2f%2fbarshan.is-a.dev%2fposts%2f2025-11-19-transfomers%2f&amp;hashtags=RAG%2cretrieval-augmented-generation%2cembeddings%2cvector-search%2cAI">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M512 62.554 L 512 449.446 C 512 483.97 483.97 512 449.446 512 L 62.554 512 C 28.03 512 0 483.97 0 449.446 L 0 62.554 C 0 28.03 28.029 0 62.554 0 L 449.446 0 C 483.971 0 512 28.03 512 62.554 Z M 269.951 190.75 L 182.567 75.216 L 56 75.216 L 207.216 272.95 L 63.9 436.783 L 125.266 436.783 L 235.9 310.383 L 332.567 436.783 L 456 436.783 L 298.367 228.367 L 432.367 75.216 L 371.033 75.216 Z M 127.633 110 L 164.101 110 L 383.481 400.065 L 349.5 400.065 Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Before We Hop Onto TRANSFORMERS  on linkedin"
            href="https://www.linkedin.com/shareArticle?mini=true&amp;url=https%3a%2f%2fbarshan.is-a.dev%2fposts%2f2025-11-19-transfomers%2f&amp;title=Before%20We%20Hop%20Onto%20TRANSFORMERS%20&amp;summary=Before%20We%20Hop%20Onto%20TRANSFORMERS%20&amp;source=https%3a%2f%2fbarshan.is-a.dev%2fposts%2f2025-11-19-transfomers%2f">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-288.985,423.278l0,-225.717l-75.04,0l0,225.717l75.04,0Zm270.539,0l0,-129.439c0,-69.333 -37.018,-101.586 -86.381,-101.586c-39.804,0 -57.634,21.891 -67.617,37.266l0,-31.958l-75.021,0c0.995,21.181 0,225.717 0,225.717l75.02,0l0,-126.056c0,-6.748 0.486,-13.492 2.474,-18.315c5.414,-13.475 17.767,-27.434 38.494,-27.434c27.135,0 38.007,20.707 38.007,51.037l0,120.768l75.024,0Zm-307.552,-334.556c-25.674,0 -42.448,16.879 -42.448,39.002c0,21.658 16.264,39.002 41.455,39.002l0.484,0c26.165,0 42.452,-17.344 42.452,-39.002c-0.485,-22.092 -16.241,-38.954 -41.943,-39.002Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Before We Hop Onto TRANSFORMERS  on reddit"
            href="https://reddit.com/submit?url=https%3a%2f%2fbarshan.is-a.dev%2fposts%2f2025-11-19-transfomers%2f&title=Before%20We%20Hop%20Onto%20TRANSFORMERS%20">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-3.446,265.638c0,-22.964 -18.616,-41.58 -41.58,-41.58c-11.211,0 -21.361,4.457 -28.841,11.666c-28.424,-20.508 -67.586,-33.757 -111.204,-35.278l18.941,-89.121l61.884,13.157c0.756,15.734 13.642,28.29 29.56,28.29c16.407,0 29.706,-13.299 29.706,-29.701c0,-16.403 -13.299,-29.702 -29.706,-29.702c-11.666,0 -21.657,6.792 -26.515,16.578l-69.105,-14.69c-1.922,-0.418 -3.939,-0.042 -5.585,1.036c-1.658,1.073 -2.811,2.761 -3.224,4.686l-21.152,99.438c-44.258,1.228 -84.046,14.494 -112.837,35.232c-7.468,-7.164 -17.589,-11.591 -28.757,-11.591c-22.965,0 -41.585,18.616 -41.585,41.58c0,16.896 10.095,31.41 24.568,37.918c-0.639,4.135 -0.99,8.328 -0.99,12.576c0,63.977 74.469,115.836 166.33,115.836c91.861,0 166.334,-51.859 166.334,-115.836c0,-4.218 -0.347,-8.387 -0.977,-12.493c14.564,-6.47 24.735,-21.034 24.735,-38.001Zm-119.474,108.193c-20.27,20.241 -59.115,21.816 -70.534,21.816c-11.428,0 -50.277,-1.575 -70.522,-21.82c-3.007,-3.008 -3.007,-7.882 0,-10.889c3.003,-2.999 7.882,-3.003 10.885,0c12.777,12.781 40.11,17.317 59.637,17.317c19.522,0 46.86,-4.536 59.657,-17.321c3.016,-2.999 7.886,-2.995 10.885,0.008c3.008,3.011 3.003,7.882 -0.008,10.889Zm-5.23,-48.781c-16.373,0 -29.701,-13.324 -29.701,-29.698c0,-16.381 13.328,-29.714 29.701,-29.714c16.378,0 29.706,13.333 29.706,29.714c0,16.374 -13.328,29.698 -29.706,29.698Zm-160.386,-29.702c0,-16.381 13.328,-29.71 29.714,-29.71c16.369,0 29.689,13.329 29.689,29.71c0,16.373 -13.32,29.693 -29.689,29.693c-16.386,0 -29.714,-13.32 -29.714,-29.693Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Before We Hop Onto TRANSFORMERS  on facebook"
            href="https://facebook.com/sharer/sharer.php?u=https%3a%2f%2fbarshan.is-a.dev%2fposts%2f2025-11-19-transfomers%2f">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-106.468,0l0,-192.915l66.6,0l12.672,-82.621l-79.272,0l0,-53.617c0,-22.603 11.073,-44.636 46.58,-44.636l36.042,0l0,-70.34c0,0 -32.71,-5.582 -63.982,-5.582c-65.288,0 -107.96,39.569 -107.96,111.204l0,62.971l-72.573,0l0,82.621l72.573,0l0,192.915l-191.104,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Before We Hop Onto TRANSFORMERS  on whatsapp"
            href="https://api.whatsapp.com/send?text=Before%20We%20Hop%20Onto%20TRANSFORMERS%20%20-%20https%3a%2f%2fbarshan.is-a.dev%2fposts%2f2025-11-19-transfomers%2f">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-58.673,127.703c-33.842,-33.881 -78.847,-52.548 -126.798,-52.568c-98.799,0 -179.21,80.405 -179.249,179.234c-0.013,31.593 8.241,62.428 23.927,89.612l-25.429,92.884l95.021,-24.925c26.181,14.28 55.659,21.807 85.658,21.816l0.074,0c98.789,0 179.206,-80.413 179.247,-179.243c0.018,-47.895 -18.61,-92.93 -52.451,-126.81Zm-126.797,275.782l-0.06,0c-26.734,-0.01 -52.954,-7.193 -75.828,-20.767l-5.441,-3.229l-56.386,14.792l15.05,-54.977l-3.542,-5.637c-14.913,-23.72 -22.791,-51.136 -22.779,-79.287c0.033,-82.142 66.867,-148.971 149.046,-148.971c39.793,0.014 77.199,15.531 105.329,43.692c28.128,28.16 43.609,65.592 43.594,105.4c-0.034,82.149 -66.866,148.983 -148.983,148.984Zm81.721,-111.581c-4.479,-2.242 -26.499,-13.075 -30.604,-14.571c-4.105,-1.495 -7.091,-2.241 -10.077,2.241c-2.986,4.483 -11.569,14.572 -14.182,17.562c-2.612,2.988 -5.225,3.364 -9.703,1.12c-4.479,-2.241 -18.91,-6.97 -36.017,-22.23c-13.314,-11.876 -22.304,-26.542 -24.916,-31.026c-2.612,-4.484 -0.279,-6.908 1.963,-9.14c2.016,-2.007 4.48,-5.232 6.719,-7.847c2.24,-2.615 2.986,-4.484 4.479,-7.472c1.493,-2.99 0.747,-5.604 -0.374,-7.846c-1.119,-2.241 -10.077,-24.288 -13.809,-33.256c-3.635,-8.733 -7.327,-7.55 -10.077,-7.688c-2.609,-0.13 -5.598,-0.158 -8.583,-0.158c-2.986,0 -7.839,1.121 -11.944,5.604c-4.105,4.484 -15.675,15.32 -15.675,37.364c0,22.046 16.048,43.342 18.287,46.332c2.24,2.99 31.582,48.227 76.511,67.627c10.685,4.615 19.028,7.371 25.533,9.434c10.728,3.41 20.492,2.929 28.209,1.775c8.605,-1.285 26.499,-10.833 30.231,-21.295c3.732,-10.464 3.732,-19.431 2.612,-21.298c-1.119,-1.869 -4.105,-2.99 -8.583,-5.232Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Before We Hop Onto TRANSFORMERS  on telegram"
            href="https://telegram.me/share/url?text=Before%20We%20Hop%20Onto%20TRANSFORMERS%20&amp;url=https%3a%2f%2fbarshan.is-a.dev%2fposts%2f2025-11-19-transfomers%2f">
            <svg version="1.1" xml:space="preserve" viewBox="2 2 28 28" height="30px" width="30px" fill="currentColor">
                <path
                    d="M26.49,29.86H5.5a3.37,3.37,0,0,1-2.47-1,3.35,3.35,0,0,1-1-2.47V5.48A3.36,3.36,0,0,1,3,3,3.37,3.37,0,0,1,5.5,2h21A3.38,3.38,0,0,1,29,3a3.36,3.36,0,0,1,1,2.46V26.37a3.35,3.35,0,0,1-1,2.47A3.38,3.38,0,0,1,26.49,29.86Zm-5.38-6.71a.79.79,0,0,0,.85-.66L24.73,9.24a.55.55,0,0,0-.18-.46.62.62,0,0,0-.41-.17q-.08,0-16.53,6.11a.59.59,0,0,0-.41.59.57.57,0,0,0,.43.52l4,1.24,1.61,4.83a.62.62,0,0,0,.63.43.56.56,0,0,0,.4-.17L16.54,20l4.09,3A.9.9,0,0,0,21.11,23.15ZM13.8,20.71l-1.21-4q8.72-5.55,8.78-5.55c.15,0,.23,0,.23.16a.18.18,0,0,1,0,.06s-2.51,2.3-7.52,6.8Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Before We Hop Onto TRANSFORMERS  on ycombinator"
            href="https://news.ycombinator.com/submitlink?t=Before%20We%20Hop%20Onto%20TRANSFORMERS%20&u=https%3a%2f%2fbarshan.is-a.dev%2fposts%2f2025-11-19-transfomers%2f">
            <svg version="1.1" xml:space="preserve" width="30px" height="30px" viewBox="0 0 512 512" fill="currentColor"
                xmlns:inkscape="http://www.inkscape.org/namespaces/inkscape">
                <path
                    d="M449.446 0C483.971 0 512 28.03 512 62.554L512 449.446C512 483.97 483.97 512 449.446 512L62.554 512C28.03 512 0 483.97 0 449.446L0 62.554C0 28.03 28.029 0 62.554 0L449.446 0ZM183.8767 87.9921H121.8427L230.6673 292.4508V424.0079H281.3328V292.4508L390.1575 87.9921H328.1233L256 238.2489z" />
            </svg>
        </a>
    </li>
</ul>

  </footer>
</article>
    </main>
    
<footer class="footer">
    <span>&copy; 2026 <a href="https://barshan.is-a.dev/">bluee :)</a></span>
    <span>
        Powered by
        <a href="https://gohugo.io/" rel="noopener noreferrer" target="_blank">Hugo</a> &
        <a href="https://github.com/adityatelange/hugo-PaperMod/" rel="noopener" target="_blank">PaperMod</a>
    </span>
</footer>
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)" class="top-link" id="top-link" accesskey="g">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
        <path d="M12 6H0l6-6z" />
    </svg>
</a>

<script>
    let menu = document.getElementById('menu')
    if (menu) {
        menu.scrollLeft = localStorage.getItem("menu-scroll-position");
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };

</script>
<script>
    document.getElementById("theme-toggle").addEventListener("click", () => {
        if (document.body.className.includes("dark")) {
            document.body.classList.remove('dark');
            localStorage.setItem("pref-theme", 'light');
        } else {
            document.body.classList.add('dark');
            localStorage.setItem("pref-theme", 'dark');
        }
    })

</script>
<script>
    document.querySelectorAll('pre > code').forEach((codeblock) => {
        const container = codeblock.parentNode.parentNode;

        const copybutton = document.createElement('button');
        copybutton.classList.add('copy-code');
        copybutton.innerHTML = 'copy';

        function copyingDone() {
            copybutton.innerHTML = 'copied!';
            setTimeout(() => {
                copybutton.innerHTML = 'copy';
            }, 2000);
        }

        copybutton.addEventListener('click', (cb) => {
            if ('clipboard' in navigator) {
                navigator.clipboard.writeText(codeblock.textContent);
                copyingDone();
                return;
            }

            const range = document.createRange();
            range.selectNodeContents(codeblock);
            const selection = window.getSelection();
            selection.removeAllRanges();
            selection.addRange(range);
            try {
                document.execCommand('copy');
                copyingDone();
            } catch (e) { };
            selection.removeRange(range);
        });

        if (container.classList.contains("highlight")) {
            container.appendChild(copybutton);
        } else if (container.parentNode.firstChild == container) {
            
        } else if (codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName == "TABLE") {
            
            codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(copybutton);
        } else {
            
            codeblock.parentNode.appendChild(copybutton);
        }
    });
</script>
</body>

</html>
